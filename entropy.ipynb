{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía de una variable aleatoria discreta:\n",
    "\n",
    "La entropía se utiliza para medir la incertidumbre o la sorpresa asociada con un evento o una variable aleatoria. En términos sencillos, representa cuánta información se necesita para describir o predecir un evento o variable aleatoria. La entropía máxima ocurre cuando todos los resultados son igualmente probables, y la entropía es igual al logaritmo en base 2 del número de resultados posibles. La entropía es mínima (cero) cuando solo hay un resultado posible (probabilidad 1) y, por lo tanto, no hay incertidumbre. \n",
    "\n",
    "La entropía se puede interpretar como la cantidad promedio de bits necesarios para codificar la información de la variable aleatoria X. Cuanto mayor sea la entropía, más incertidumbre hay y más bits se necesitan para describir la variable.\n",
    "\n",
    "La fórmula típica para calcular la entropía de una variable aleatoria discreta X con un conjunto de posibles resultados es:\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= -\\sum_{x \\in X} p(x) \\log p(x) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se calculan las entropías de distintos conjunto de datos que fueron extraídos de la naturaleza, hojas de 2 clases de arbustos. Se dispone de las medidas de los anchos y largos de ambas clases de hojas (C1 y C2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(prob: np.array) -> float:\n",
    "    H = 0\n",
    "    for i in range(prob.shape[0]):\n",
    "        if prob[i] != 0:\n",
    "            H += prob[i] * np.log2(prob[i])\n",
    "    return -H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se lee el archivo CSV\n",
    "data = pd.read_csv('DatasetHojas.csv', sep=',', header=0)\n",
    "data_clase1 = data[data['Clase'] == 1]\n",
    "data_clase2 = data[data['Clase'] == 2]\n",
    "\n",
    "# Se separan los datos de cada clase en sus respectivas variables\n",
    "largo_clase1 = data_clase1['Largo']\n",
    "ancho_clase1 = data_clase1['Ancho']\n",
    "largo_clase2 = data_clase2['Largo']\n",
    "ancho_clase2 = data_clase2['Ancho']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía de ancho de clase 1, H(A1):  1.84155491187079\n",
      "Entropía de ancho de clase 2, H(A2):  1.7906927146926774\n",
      "Entropía de largo de clase 1, H(L1):  1.7206031643621587\n",
      "Entropía de largo de clase 2, H(L2):  1.728481135079078\n"
     ]
    }
   ],
   "source": [
    "n_bins = 5\n",
    "\n",
    "hist_ancho_clase1 = np.histogram(ancho_clase1, bins=n_bins)\n",
    "hist_ancho_clase2 = np.histogram(ancho_clase2, bins=n_bins)\n",
    "hist_largo_clase1 = np.histogram(largo_clase1, bins=n_bins)\n",
    "hist_largo_clase2 = np.histogram(largo_clase2, bins=n_bins)\n",
    "\n",
    "prob_ancho_clase1 = hist_ancho_clase1[0] / np.sum(hist_ancho_clase1[0])\n",
    "prob_ancho_clase2 = hist_ancho_clase2[0] / np.sum(hist_ancho_clase2[0])\n",
    "prob_largo_clase1 = hist_largo_clase1[0] / np.sum(hist_largo_clase1[0])\n",
    "prob_largo_clase2 = hist_largo_clase2[0] / np.sum(hist_largo_clase2[0])\n",
    "\n",
    "\n",
    "print(\"Entropía de ancho de clase 1, H(A1): \", entropy(prob_ancho_clase1))\n",
    "print(\"Entropía de ancho de clase 2, H(A2): \", entropy(prob_ancho_clase2))\n",
    "print(\"Entropía de largo de clase 1, H(L1): \", entropy(prob_largo_clase1))\n",
    "print(\"Entropía de largo de clase 2, H(L2): \", entropy(prob_largo_clase2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía conjunta de un par de variables aleatorias discretas:\n",
    "\n",
    "La entropía conjunta mide cuánta incertidumbre hay en la combinación de los valores de X e Y. Si la entropía conjunta es alta, significa que la combinación de X e Y es altamente incierta y contiene mucha información. Si la entropía conjunta es baja, implica que la combinación de X e Y es predecible o que las variables están relacionadas de alguna manera, lo que introduce una noción de independencia entre los datos.\n",
    "\n",
    "A continuación se detalla la fórmula:\n",
    "\n",
    "$$ H(X,Y) = - \\sum_{x \\in X} \\sum_{y \\in Y} \\ p(x,y) \\log p(x,y) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía conjunta clase 1, H(A1,L1) = 3.0321465454645993\n",
      "Entropía conjunta clase 2, H(A2,L2) =: 2.853608585978931\n"
     ]
    }
   ],
   "source": [
    "hist_clase1, _, _ = np.histogram2d(largo_clase1, ancho_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "\n",
    "hist_clase2, _, _ = np.histogram2d(largo_clase2, ancho_clase2, bins=5)\n",
    "joint_prob_clase2 = hist_clase2 / np.sum(hist_clase2)\n",
    "\n",
    "def joint_entropy(prob: np.ndarray) -> float:\n",
    "    \"\"\" Calcula la entropía conjunta de una matriz de probabilidades \"\"\"\n",
    "    H = 0\n",
    "    for i in range(prob.shape[0]):\n",
    "        for j in range(prob.shape[1]):\n",
    "            if prob[i][j] != 0:\n",
    "                H += prob[i][j] * np.log2(prob[i][j])\n",
    "    return -H\n",
    "\n",
    "print('Entropía conjunta clase 1, H(A1,L1) =',joint_entropy(joint_prob_clase1))\n",
    "print('Entropía conjunta clase 2, H(A2,L2) =:',joint_entropy(joint_prob_clase2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía condicional\n",
    "\n",
    "La entropía condicional mide cuánta información adicional se necesita para describir $Y$\n",
    "dado que ya se conoce $X$. Si $H(Y|X)$ es alto, significa que $Y$ es altamente incierto o independiente de $X$, en cambio si es bajo, podemos decir que $Y$ es predecible o dependiente de $X$.\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y|X) &= \\sum_{x \\in X} \\ p(x) \\, H(Y|X = x) \\\\\n",
    "&= - \\sum_{x \\in X} \\ p(x) \\sum_{y \\in Y} \\ p(y|x) \\log p(y|x) \\\\\n",
    "&= - \\sum_{x \\in X} \\sum_{y \\in Y} \\ p(x,y) \\log p(y|x) \\\\\n",
    "&= - E \\, \\log p(Y|X) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía condicional clase 1, (A1|L1): 1.3115433811024402\n",
      "Entropía condicional clase 2, (A2|L2): 1.1251274508998534\n"
     ]
    }
   ],
   "source": [
    "def conditional_distribution(joint_probability: np.ndarray)-> np.ndarray:\n",
    "    \"\"\" Calcula la probabilidad condicional dada la probabilidad conjunta \"\"\"\n",
    "    conditional_prob = np.zeros(joint_probability.shape)\n",
    "    for i in range(joint_probability.shape[0]):\n",
    "        for j in range(joint_probability.shape[1]):\n",
    "            conditional_prob[i][j] = joint_probability[i][j] / np.sum(joint_probability[i])\n",
    "    return conditional_prob\n",
    "\n",
    "conditional_prob_clase1 = conditional_distribution(joint_prob_clase1)\n",
    "conditional_prob_clase2 = conditional_distribution(joint_prob_clase2)\n",
    "\n",
    "def conditional_entropy(conditional_prob: np.ndarray, joint_prob: np.ndarray) -> float:\n",
    "    \"\"\" Calcula la entropía condicional de una matriz de probabilidades \"\"\"\n",
    "    H = 0\n",
    "    for i in range(conditional_prob.shape[0]):\n",
    "        for j in range(conditional_prob.shape[1]):\n",
    "            if conditional_prob[i][j] != 0:\n",
    "                H += joint_prob[i][j] * np.log2(conditional_prob[i][j])\n",
    "    return -H\n",
    "\n",
    "print('Entropía condicional clase 1, (A1|L1):',conditional_entropy(conditional_prob_clase1, joint_prob_clase1))\n",
    "print('Entropía condicional clase 2, (A2|L2):',conditional_entropy(conditional_prob_clase2, joint_prob_clase2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora las condicionales pero entre los anchos $H(A_1 | A_2)$ y los largos $H(L_1 | L_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancho1 = np.array(ancho_clase1)[:ancho_clase2.shape[0]]\n",
    "largo1 = np.array(largo_clase1)[:largo_clase2.shape[0]]\n",
    "\n",
    "hist_anchos, _, _ = np.histogram2d(ancho_clase2, ancho1, bins=5)\n",
    "joint_prob_anchos = hist_anchos / np.sum(hist_anchos)\n",
    "\n",
    "hist_largos, _, _ = np.histogram2d(largo_clase2, largo1, bins=5)\n",
    "joint_prob_largos = hist_largos / np.sum(hist_largos)\n",
    "\n",
    "# print('Entropía conjunta anchos, H(A1, A2) =',joint_entropy(joint_prob_anchos))\n",
    "# print('Entropía conjunta largos, H(L1, L2) =',joint_entropy(joint_prob_largos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía condicional anchos, H(A1|A2) = 1.8468602935137588\n",
      "Entropía condicional largos, H(L1|L2) = 1.8522168000712587\n"
     ]
    }
   ],
   "source": [
    "conditional_prob_anchos = conditional_distribution(joint_prob_anchos)\n",
    "conditional_prob_largos = conditional_distribution(joint_prob_largos)\n",
    "\n",
    "print('Entropía condicional anchos, H(A1|A2) =',conditional_entropy(conditional_prob_anchos, joint_prob_anchos))\n",
    "print('Entropía condicional largos, H(L1|L2) =',conditional_entropy(conditional_prob_largos, joint_prob_largos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La información mutua entre dos variables aleatorias discretas $I(X;Y)$ es la entropía relativa entre la probabilidad conjunta y el producto de las probabilidades marginales:\n",
    "\n",
    "La información mutua, denotada como $I(X;Y)$, es un concepto importante en la teoría de la información y la teoría de la probabilidad que mide cuánta información comparten o cuánta dependencia mutua existe entre dos variables aleatorias, \n",
    "$X$ e $Y$. La información mutua se utiliza para cuantificar cuánta información se gana al observar una de las variables cuando se conoce la otra.\n",
    "\\begin{aligned}\n",
    "I(X;Y) &= \\sum_{x \\in X} \\sum_{y \\in Y} \\ p(x,y) \\log \\frac{p(x,y)}{p(x) p(y)} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "##### Por regla de la cadena, se puede escribir como:\n",
    "$$ I(X;Y) = H(X) - H(X|Y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua clase 1, I(A1; L1) = 0.5300115307683494\n",
      "Información mutua anchos, I(A1; A2) = 0.0796008638042334\n"
     ]
    }
   ],
   "source": [
    "def mutual_information(marg_prob_X, marg_prob_Y, joint_prob) -> float:\n",
    "    I = 0\n",
    "    for x in range(marg_prob_X.shape[0]):\n",
    "        for y in range(marg_prob_Y.shape[0]):\n",
    "            if joint_prob[x][y] != 0:\n",
    "                I += joint_prob[x][y] * np.log2(joint_prob[x][y] / (marg_prob_X[x] * marg_prob_Y[y]))\n",
    "    return I\n",
    "\n",
    "hist_clase1, _, _ = np.histogram2d(largo_clase1, ancho_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "\n",
    "marginal_largo_clase1 = np.sum(joint_prob_clase1, axis=1)\n",
    "marginal_ancho_clase1 = np.sum(joint_prob_clase1, axis=0)\n",
    "\n",
    "print('Información mutua clase 1, I(A1; L1) =',mutual_information(marginal_largo_clase1, marginal_ancho_clase1, joint_prob_clase1))\n",
    "\n",
    "# Lo mismo para ancho1 y ancho2\n",
    "ancho1 = np.array(ancho_clase1)[:ancho_clase2.shape[0]]\n",
    "ancho2 = ancho_clase2\n",
    "\n",
    "hist_anchos, _, _ = np.histogram2d(ancho1, ancho2, bins=5)\n",
    "joint_prob_anchos = hist_anchos / np.sum(hist_anchos)\n",
    "\n",
    "marginal_ancho1 = np.sum(joint_prob_anchos, axis=1)\n",
    "marginal_ancho2 = np.sum(joint_prob_anchos, axis=0)\n",
    "\n",
    "print('Información mutua anchos, I(A1; A2) =',mutual_information(marginal_ancho1, marginal_ancho2, joint_prob_anchos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua I(L1;A1) = 0.5300115307683496\n",
      "Información mutua H(L1) - H(L1|A1) = 0.5300115307683493\n",
      "La información mutua I(L1;A1) es igual a la diferencia entre la entropía de L1 y la entropía condicional de L1 dado A1 \n",
      "\n",
      "Información mutua I(L2;A2) = 0.6655652637928242\n",
      "Información mutua H(L2) - H(L2|A2) = 0.6655652637928238\n",
      "La información mutua I(L2;A2) es igual a la diferencia entre la entropía de L2 y la entropía condicional de L2 dado A2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Corroboración\n",
    "hist_clase1, _, _ = np.histogram2d(ancho_clase1, largo_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "conditional_prob_clase1 = conditional_distribution(joint_prob_clase1)\n",
    "I_clase_1 = entropy(prob_largo_clase1) - conditional_entropy(conditional_prob_clase1, joint_prob_clase1)\n",
    "print('Información mutua I(L1;A1) =',mutual_information(prob_ancho_clase1, prob_largo_clase1, joint_prob_clase1))\n",
    "print('Información mutua H(L1) - H(L1|A1) =', I_clase_1)\n",
    "#quedarse con los 3 primeros decimales\n",
    "if round(I_clase_1, 3) == round(mutual_information(prob_ancho_clase1, prob_largo_clase1, joint_prob_clase1), 3):\n",
    "    print('La información mutua I(L1;A1) es igual a la diferencia entre la entropía de L1 y la entropía condicional de L1 dado A1 \\n')\n",
    "\n",
    "hist_clase2, _, _ = np.histogram2d(ancho_clase2, largo_clase2, bins=5)\n",
    "joint_prob_clase2 = hist_clase2 / np.sum(hist_clase2)\n",
    "conditional_prob_clase2 = conditional_distribution(joint_prob_clase2)\n",
    "I_clase_2 = entropy(prob_largo_clase2) - conditional_entropy(conditional_prob_clase2, joint_prob_clase2)\n",
    "print('Información mutua I(L2;A2) =', I_clase_2)\n",
    "print('Información mutua H(L2) - H(L2|A2) =',mutual_information(prob_ancho_clase2, prob_largo_clase2, joint_prob_clase2))\n",
    "if round(I_clase_2, 3) == round(mutual_information(prob_ancho_clase2, prob_largo_clase2, joint_prob_clase2), 3):\n",
    "    print('La información mutua I(L2;A2) es igual a la diferencia entre la entropía de L2 y la entropía condicional de L2 dado A2 \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía relativa o distancia de Kullback-Leibler entre dos distribuciones de probabilidad $p$ y $q$:\n",
    "\n",
    "La entropía relativa, también conocida como la distancia de Kullback-Leibler (KL), es una medida que cuantifica la diferencia o la divergencia entre dos distribuciones de probabilidad, $p(x)$ y $q(x)$. Esta medida se utiliza para determinar cuánta información se pierde cuando se representa $p$ con respecto a $q$.\n",
    "\\begin{aligned}\n",
    "D_{KL}(p||q) &= \\sum_{x \\in X} \\ p(x) \\log \\frac{p(x)}{q(x)} \\\\\n",
    "&= E_p \\log \\frac{p(X)}{q(X)} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "A continuación se calcula la divergencia entre las conjuntas de las ambas clases de hojas. La fórmula es:\n",
    "\\begin{equation}\n",
    "D_{KL}(p(x, y) || q(x, y)) = \\sum_x \\sum_y p(x, y) \\log\\left(\\frac{p(x, y)}{q(x, y)}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_entropy(P: np.ndarray, Q: np.ndarray) -> float:\n",
    "    D = 0\n",
    "    for p in range(P.shape[0]):\n",
    "        for q in range(Q.shape[1]):\n",
    "            if P[p][q] != 0 and Q[p][q] != 0:\n",
    "                D += P[p][q] * np.log2(P[p][q] / Q[p][q])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía relativa entre la conjunta de las clases, D(C1||C2) = 0.24180405291993165\n"
     ]
    }
   ],
   "source": [
    "print('Entropía relativa entre la conjunta de las clases, D(C1||C2) =',relative_entropy(joint_prob_clase1, joint_prob_clase2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
