{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía de una variable aleatoria discreta:\n",
    "\n",
    "La entropía se utiliza para medir la incertidumbre o la sorpresa asociada con un evento o una variable aleatoria. En términos sencillos, representa cuánta información se necesita para describir o predecir un evento o variable aleatoria. La entropía máxima ocurre cuando todos los resultados son igualmente probables, y la entropía es igual al logaritmo en base 2 del número de resultados posibles. La entropía es mínima (cero) cuando solo hay un resultado posible (probabilidad 1) y, por lo tanto, no hay incertidumbre. \n",
    "\n",
    "La entropía se puede interpretar como la cantidad promedio de bits necesarios para codificar la información de la variable aleatoria X. Cuanto mayor sea la entropía, más incertidumbre hay y más bits se necesitan para describir la variable.\n",
    "\n",
    "La fórmula típica para calcular la entropía de una variable aleatoria discreta X con un conjunto de posibles resultados es:\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= -\\sum_{x \\in X} p(x) \\log p(x) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(prob: np.array) -> float:\n",
    "    H = 0\n",
    "    for i in range(prob.shape[0]):\n",
    "        if prob[i] != 0:\n",
    "            H += prob[i] * np.log2(prob[i])\n",
    "    return -H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se lee el archivo CSV\n",
    "data = pd.read_csv('DatasetHojas.csv', sep=',', header=0)\n",
    "data_clase1 = data[data['Clase'] == 1]\n",
    "data_clase2 = data[data['Clase'] == 2]\n",
    "\n",
    "# Se separan los datos de cada clase en sus respectivas variables\n",
    "largo_clase1 = data_clase1['Largo']\n",
    "ancho_clase1 = data_clase1['Ancho']\n",
    "largo_clase2 = data_clase2['Largo']\n",
    "ancho_clase2 = data_clase2['Ancho']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía de ancho de clase 1, H(A1):  1.84155491187079\n",
      "Entropía de ancho de clase 2, H(A2):  1.7906927146926774\n",
      "Entropía de largo de clase 1, H(L1):  1.7206031643621587\n",
      "Entropía de largo de clase 2, H(L2):  1.728481135079078\n"
     ]
    }
   ],
   "source": [
    "n_bins = 5\n",
    "\n",
    "hist_ancho_clase1 = np.histogram(ancho_clase1, bins=n_bins)\n",
    "hist_ancho_clase2 = np.histogram(ancho_clase2, bins=n_bins)\n",
    "hist_largo_clase1 = np.histogram(largo_clase1, bins=n_bins)\n",
    "hist_largo_clase2 = np.histogram(largo_clase2, bins=n_bins)\n",
    "\n",
    "prob_ancho_clase1 = hist_ancho_clase1[0] / np.sum(hist_ancho_clase1[0])\n",
    "prob_ancho_clase2 = hist_ancho_clase2[0] / np.sum(hist_ancho_clase2[0])\n",
    "prob_largo_clase1 = hist_largo_clase1[0] / np.sum(hist_largo_clase1[0])\n",
    "prob_largo_clase2 = hist_largo_clase2[0] / np.sum(hist_largo_clase2[0])\n",
    "\n",
    "\n",
    "print(\"Entropía de ancho de clase 1, H(A1): \", entropy(prob_ancho_clase1))\n",
    "print(\"Entropía de ancho de clase 2, H(A2): \", entropy(prob_ancho_clase2))\n",
    "print(\"Entropía de largo de clase 1, H(L1): \", entropy(prob_largo_clase1))\n",
    "print(\"Entropía de largo de clase 2, H(L2): \", entropy(prob_largo_clase2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía conjunta de un par de variables aleatorias discretas: $H(X,Y)$\n",
    "$$ H(X,Y) = - \\sum_{x \\in X} \\sum_{y \\in Y} \\ p(x,y) \\log p(x,y) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía conjunta clase 1, H(A1,L1) = 3.0321465454645993\n",
      "Entropía conjunta clase 2, H(A2,L2) =: 2.853608585978931\n"
     ]
    }
   ],
   "source": [
    "hist_clase1, _, _ = np.histogram2d(largo_clase1, ancho_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "\n",
    "hist_clase2, _, _ = np.histogram2d(largo_clase2, ancho_clase2, bins=5)\n",
    "joint_prob_clase2 = hist_clase2 / np.sum(hist_clase2)\n",
    "\n",
    "def joint_entropy(prob: np.ndarray) -> float:\n",
    "    \"\"\" Calcula la entropía conjunta de una matriz de probabilidades \"\"\"\n",
    "    H = 0\n",
    "    for i in range(prob.shape[0]):\n",
    "        for j in range(prob.shape[1]):\n",
    "            if prob[i][j] != 0:\n",
    "                H += prob[i][j] * np.log2(prob[i][j])\n",
    "    return -H\n",
    "\n",
    "print('Entropía conjunta clase 1, H(A1,L1) =',joint_entropy(joint_prob_clase1))\n",
    "print('Entropía conjunta clase 2, H(A2,L2) =:',joint_entropy(joint_prob_clase2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía condicional\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y|X) &= \\sum_{x \\in X} \\ p(x) \\, H(Y|X = x) \\\\\n",
    "&= - \\sum_{x \\in X} \\ p(x) \\sum_{y \\in Y} \\ p(y|x) \\log p(y|x) \\\\\n",
    "&= - \\sum_{x \\in X} \\sum_{y \\in Y} \\ p(x,y) \\log p(y|x) \\\\\n",
    "&= - E \\, \\log p(Y|X) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía condicional clase 1, (A1|L1): 1.3115433811024402\n",
      "Entropía condicional clase 2, (A2|L2): 1.1251274508998534\n"
     ]
    }
   ],
   "source": [
    "def conditional_distribution(joint_probability: np.ndarray)-> np.ndarray:\n",
    "    \"\"\" Calcula la probabilidad condicional dada la probabilidad conjunta \"\"\"\n",
    "    conditional_prob = np.zeros(joint_probability.shape)\n",
    "    for i in range(joint_probability.shape[0]):\n",
    "        for j in range(joint_probability.shape[1]):\n",
    "            conditional_prob[i][j] = joint_probability[i][j] / np.sum(joint_probability[i])\n",
    "    return conditional_prob\n",
    "\n",
    "conditional_prob_clase1 = conditional_distribution(joint_prob_clase1)\n",
    "conditional_prob_clase2 = conditional_distribution(joint_prob_clase2)\n",
    "\n",
    "def conditional_entropy(conditional_prob: np.ndarray, joint_prob: np.ndarray) -> float:\n",
    "    \"\"\" Calcula la entropía condicional de una matriz de probabilidades \"\"\"\n",
    "    H = 0\n",
    "    for i in range(conditional_prob.shape[0]):\n",
    "        for j in range(conditional_prob.shape[1]):\n",
    "            if conditional_prob[i][j] != 0:\n",
    "                H += joint_prob[i][j] * np.log2(conditional_prob[i][j])\n",
    "    return -H\n",
    "\n",
    "print('Entropía condicional clase 1, (A1|L1):',conditional_entropy(conditional_prob_clase1, joint_prob_clase1))\n",
    "print('Entropía condicional clase 2, (A2|L2):',conditional_entropy(conditional_prob_clase2, joint_prob_clase2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora las condicionales pero entre los anchos $H(A_1 | A_2)$ y los largos $H(L_1 | L_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía conjunta anchos, H(A1, A2) = 3.6375530082064356\n",
      "Entropía conjunta largos, H(L1, L2) = 3.5806979351503374\n"
     ]
    }
   ],
   "source": [
    "ancho1 = np.array(ancho_clase1)[:ancho_clase2.shape[0]]\n",
    "largo1 = np.array(largo_clase1)[:largo_clase2.shape[0]]\n",
    "\n",
    "hist_anchos, _, _ = np.histogram2d(ancho_clase2, ancho1, bins=5)\n",
    "joint_prob_anchos = hist_anchos / np.sum(hist_anchos)\n",
    "\n",
    "hist_largos, _, _ = np.histogram2d(largo_clase2, largo1, bins=5)\n",
    "joint_prob_largos = hist_largos / np.sum(hist_largos)\n",
    "\n",
    "print('Entropía conjunta anchos, H(A1, A2) =',joint_entropy(joint_prob_anchos))\n",
    "print('Entropía conjunta largos, H(L1, L2) =',joint_entropy(joint_prob_largos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía condicional anchos, H(A1|A2) = 1.8468602935137588\n",
      "Entropía condicional largos, H(L1|L2) = 1.8522168000712587\n"
     ]
    }
   ],
   "source": [
    "conditional_prob_anchos = conditional_distribution(joint_prob_anchos)\n",
    "conditional_prob_largos = conditional_distribution(joint_prob_largos)\n",
    "\n",
    "print('Entropía condicional anchos, H(A1|A2) =',conditional_entropy(conditional_prob_anchos, joint_prob_anchos))\n",
    "print('Entropía condicional largos, H(L1|L2) =',conditional_entropy(conditional_prob_largos, joint_prob_largos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fd9b5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fd9b5_level0_col0\" class=\"col_heading level0 col0\" >A1</th>\n",
       "      <th id=\"T_fd9b5_level0_col1\" class=\"col_heading level0 col1\" >A2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9b5_level0_row0\" class=\"row_heading level0 row0\" >L1</th>\n",
       "      <td id=\"T_fd9b5_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_fd9b5_row0_col1\" class=\"data row0 col1\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9b5_level0_row1\" class=\"row_heading level0 row1\" >L2</th>\n",
       "      <td id=\"T_fd9b5_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_fd9b5_row1_col1\" class=\"data row1 col1\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x156c52c3910>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make data frame with\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"A1\": [1, 2],\n",
    "    \"A2\": [3, 4],\n",
    "})\n",
    "df.style \\\n",
    "  .format(precision=3, thousands=\".\", decimal=\",\") \\\n",
    "  .format_index(str.upper, axis=1) \\\n",
    "  .relabel_index([\"L1\", \"L2\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La información mutua entre dos variables aleatorias discretas $I(X;Y)$ es la entropía relativa entre la probabilidad conjunta y el producto de las probabilidades marginales:\n",
    "\n",
    "\\begin{aligned}\n",
    "I(X;Y) &= \\sum_{x \\in X} \\sum_{y \\in Y} \\ p(x,y) \\log \\frac{p(x,y)}{p(x) p(y)} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "##### Por regla de la cadena, se puede escribir como:\n",
    "$$ I(X;Y) = H(X) - H(X|Y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua clase 1, I(A1; L1) = 0.5300115307683494\n",
      "Información mutua anchos, I(A1; A2) = 0.0796008638042334\n"
     ]
    }
   ],
   "source": [
    "def mutual_information(marg_prob_X, marg_prob_Y, joint_prob) -> float:\n",
    "    I = 0\n",
    "    for x in range(marg_prob_X.shape[0]):\n",
    "        for y in range(marg_prob_Y.shape[0]):\n",
    "            if joint_prob[x][y] != 0:\n",
    "                I += joint_prob[x][y] * np.log2(joint_prob[x][y] / (marg_prob_X[x] * marg_prob_Y[y]))\n",
    "    return I\n",
    "\n",
    "hist_clase1, _, _ = np.histogram2d(largo_clase1, ancho_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "\n",
    "marginal_largo_clase1 = np.sum(joint_prob_clase1, axis=1)\n",
    "marginal_ancho_clase1 = np.sum(joint_prob_clase1, axis=0)\n",
    "\n",
    "print('Información mutua clase 1, I(A1; L1) =',mutual_information(marginal_largo_clase1, marginal_ancho_clase1, joint_prob_clase1))\n",
    "\n",
    "# Lo mismo para ancho1 y ancho2\n",
    "ancho1 = np.array(ancho_clase1)[:ancho_clase2.shape[0]]\n",
    "ancho2 = ancho_clase2\n",
    "\n",
    "hist_anchos, _, _ = np.histogram2d(ancho1, ancho2, bins=5)\n",
    "joint_prob_anchos = hist_anchos / np.sum(hist_anchos)\n",
    "\n",
    "marginal_ancho1 = np.sum(joint_prob_anchos, axis=1)\n",
    "marginal_ancho2 = np.sum(joint_prob_anchos, axis=0)\n",
    "\n",
    "print('Información mutua anchos, I(A1; A2) =',mutual_information(marginal_ancho1, marginal_ancho2, joint_prob_anchos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua I(L1;A1) = 0.5300115307683496\n",
      "Información mutua H(L1) - H(L1|A1) = 0.5300115307683493\n",
      "Información mutua I(L2;A2) = 0.6655652637928242\n",
      "Información mutua H(L2) - H(L2|A2) = 0.6655652637928238\n"
     ]
    }
   ],
   "source": [
    "# Corroboración\n",
    "hist_clase1, _, _ = np.histogram2d(ancho_clase1, largo_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "conditional_prob_clase1 = conditional_distribution(joint_prob_clase1)\n",
    "I_clase_1 = entropy(prob_largo_clase1) - conditional_entropy(conditional_prob_clase1, joint_prob_clase1)\n",
    "print('Información mutua I(L1;A1) =',mutual_information(prob_ancho_clase1, prob_largo_clase1, joint_prob_clase1))\n",
    "print('Información mutua H(L1) - H(L1|A1) =', I_clase_1)\n",
    "\n",
    "hist_clase2, _, _ = np.histogram2d(ancho_clase2, largo_clase2, bins=5)\n",
    "joint_prob_clase2 = hist_clase2 / np.sum(hist_clase2)\n",
    "conditional_prob_clase2 = conditional_distribution(joint_prob_clase2)\n",
    "I_clase_2 = entropy(prob_largo_clase2) - conditional_entropy(conditional_prob_clase2, joint_prob_clase2)\n",
    "print('Información mutua I(L2;A2) =', I_clase_2)\n",
    "print('Información mutua H(L2) - H(L2|A2) =',mutual_information(prob_ancho_clase2, prob_largo_clase2, joint_prob_clase2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía relativa o distancia de Kullback-Leibler entre dos distribuciones de probabilidad $p$ y $q$:\n",
    "\n",
    "\\begin{aligned}\n",
    "D(p||q) &= \\sum_{x \\in X} \\ p(x) \\log \\frac{p(x)}{q(x)} \\\\\n",
    "&= E_p \\log \\frac{p(X)}{q(X)} \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_entropy(P: np.ndarray, Q: np.ndarray) -> float:\n",
    "    D = 0\n",
    "    for p in range(P.shape[0]):\n",
    "        for q in range(Q.shape[1]):\n",
    "            if P[p][q] != 0 and Q[p][q] != 0:\n",
    "                D += P[p][q] * np.log2(P[p][q] / Q[p][q])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía relativa entre la conjunta de las clases, D(C1||C2) = 0.24180405291993165\n"
     ]
    }
   ],
   "source": [
    "print('Entropía relativa entre la conjunta de las clases, D(C1||C2) =',relative_entropy(joint_prob_clase1, joint_prob_clase2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
