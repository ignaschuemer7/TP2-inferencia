{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X = 1$ con probabilidad $p$ y $X = 0$ con probabilidad $1-p$.\n",
    "\n",
    "Luego,\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= -p \\log(p) - (1-p) \\log(1-p) \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(prob: np.array) -> float:\n",
    "    H = 0\n",
    "    for i in range(prob.shape[0]):\n",
    "        if prob[i] != 0:\n",
    "            H += prob[i] * np.log2(prob[i])\n",
    "    return -H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se lee el archivo CSV\n",
    "data = pd.read_csv('DatasetHojas.csv', sep=',', header=0)\n",
    "data_clase1 = data[data['Clase'] == 1]\n",
    "data_clase2 = data[data['Clase'] == 2]\n",
    "\n",
    "# Se separan los datos de cada clase en sus respectivas variables\n",
    "largo_clase1 = data_clase1['Largo']\n",
    "ancho_clase1 = data_clase1['Ancho']\n",
    "largo_clase2 = data_clase2['Largo']\n",
    "ancho_clase2 = data_clase2['Ancho']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía de ancho de clase 1:  1.84155491187079\n",
      "Entropía de ancho de clase 2:  1.7906927146926774\n",
      "Entropía de largo de clase 1:  1.7206031643621587\n",
      "Entropía de largo de clase 2:  1.728481135079078\n"
     ]
    }
   ],
   "source": [
    "n_bins = 5\n",
    "\n",
    "hist_ancho_clase1 = np.histogram(ancho_clase1, bins=n_bins)\n",
    "hist_ancho_clase2 = np.histogram(ancho_clase2, bins=n_bins)\n",
    "hist_largo_clase1 = np.histogram(largo_clase1, bins=n_bins)\n",
    "hist_largo_clase2 = np.histogram(largo_clase2, bins=n_bins)\n",
    "\n",
    "prob_ancho_clase1 = hist_ancho_clase1[0] / np.sum(hist_ancho_clase1[0])\n",
    "prob_ancho_clase2 = hist_ancho_clase2[0] / np.sum(hist_ancho_clase2[0])\n",
    "prob_largo_clase1 = hist_largo_clase1[0] / np.sum(hist_largo_clase1[0])\n",
    "prob_largo_clase2 = hist_largo_clase2[0] / np.sum(hist_largo_clase2[0])\n",
    "\n",
    "\n",
    "print(\"Entropía de ancho de clase 1: \", entropy(prob_ancho_clase1))\n",
    "print(\"Entropía de ancho de clase 2: \", entropy(prob_ancho_clase2))\n",
    "print(\"Entropía de largo de clase 1: \", entropy(prob_largo_clase1))\n",
    "print(\"Entropía de largo de clase 2: \", entropy(prob_largo_clase2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía conjunta \n",
    "$$ H(X,Y) = - \\sum_{x} \\sum_{y} \\ p(x,y) \\log p(x,y) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía conjunta clase 1: 3.0321465454645993\n",
      "Entropía conjunta clase 2: 2.853608585978931\n"
     ]
    }
   ],
   "source": [
    "hist_clase1, _, _ = np.histogram2d(largo_clase1, ancho_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "\n",
    "hist_clase2, _, _ = np.histogram2d(largo_clase2, ancho_clase2, bins=5)\n",
    "joint_prob_clase2 = hist_clase2 / np.sum(hist_clase2)\n",
    "\n",
    "def joint_entropy(prob: np.ndarray) -> float:\n",
    "    \"\"\" Calcula la entropía conjunta de una matriz de probabilidades \"\"\"\n",
    "    H = 0\n",
    "    for i in range(prob.shape[0]):\n",
    "        for j in range(prob.shape[1]):\n",
    "            if prob[i][j] != 0:\n",
    "                H += prob[i][j] * np.log2(prob[i][j])\n",
    "    return -H\n",
    "\n",
    "print('Entropía conjunta clase 1:',joint_entropy(joint_prob_clase1))\n",
    "print('Entropía conjunta clase 2:',joint_entropy(joint_prob_clase2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía condicional\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y|X) &= \\sum_{x} \\ p(x) H(Y|X = x) \\\\\n",
    "&= - \\sum_{x} \\ p(x) \\sum_{y} \\ p(y|x) \\log p(y|x) \\\\\n",
    "&= - \\sum_{x} \\sum_{y} \\ p(x,y) \\log p(y|x) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía condicional clase 1 (A1|L1): 1.3115433811024402\n",
      "Entropía condicional clase 2 (A2|L2): 1.1251274508998534\n"
     ]
    }
   ],
   "source": [
    "def conditional_distribution(joint_probability: np.ndarray)-> np.ndarray:\n",
    "    \"\"\" Calcula la probabilidad condicional dada la probabilidad conjunta \"\"\"\n",
    "    conditional_prob = np.zeros(joint_probability.shape)\n",
    "    for i in range(joint_probability.shape[0]):\n",
    "        for j in range(joint_probability.shape[1]):\n",
    "            conditional_prob[i][j] = joint_probability[i][j] / np.sum(joint_probability[i])\n",
    "    return conditional_prob\n",
    "\n",
    "conditional_prob_clase1 = conditional_distribution(joint_prob_clase1)\n",
    "conditional_prob_clase2 = conditional_distribution(joint_prob_clase2)\n",
    "\n",
    "def conditional_entropy(conditional_prob: np.ndarray, joint_prob: np.ndarray) -> float:\n",
    "    \"\"\" Calcula la entropía condicional de una matriz de probabilidades \"\"\"\n",
    "    H = 0\n",
    "    for i in range(conditional_prob.shape[0]):\n",
    "        for j in range(conditional_prob.shape[1]):\n",
    "            if conditional_prob[i][j] != 0:\n",
    "                H += joint_prob[i][j] * np.log2(conditional_prob[i][j])\n",
    "    return -H\n",
    "\n",
    "print('Entropía condicional clase 1 (A1|L1):',conditional_entropy(conditional_prob_clase1, joint_prob_clase1))\n",
    "print('Entropía condicional clase 2 (A2|L2):',conditional_entropy(conditional_prob_clase2, joint_prob_clase2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora las condicionales pero entre los anchos $H(A_1 | A_2)$ y los largos $H(L_1 | L_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía conjunta anchos: 3.6375530082064356\n",
      "Entropía conjunta largos: 3.5806979351503374\n"
     ]
    }
   ],
   "source": [
    "ancho1 = np.array(ancho_clase1)[:ancho_clase2.shape[0]]\n",
    "largo1 = np.array(largo_clase1)[:largo_clase2.shape[0]]\n",
    "\n",
    "hist_anchos, _, _ = np.histogram2d(ancho_clase2, ancho1, bins=5)\n",
    "joint_prob_anchos = hist_anchos / np.sum(hist_anchos)\n",
    "\n",
    "hist_largos, _, _ = np.histogram2d(largo_clase2, largo1, bins=5)\n",
    "joint_prob_largos = hist_largos / np.sum(hist_largos)\n",
    "\n",
    "print('Entropía conjunta anchos:',joint_entropy(joint_prob_anchos))\n",
    "print('Entropía conjunta largos:',joint_entropy(joint_prob_largos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía condicional anchos (A1|A2): 1.8468602935137588\n",
      "Entropía condicional largos (L1|L2): 1.8522168000712587\n"
     ]
    }
   ],
   "source": [
    "conditional_prob_anchos = conditional_distribution(joint_prob_anchos)\n",
    "conditional_prob_largos = conditional_distribution(joint_prob_largos)\n",
    "\n",
    "print('Entropía condicional anchos (A1|A2):',conditional_entropy(conditional_prob_anchos, joint_prob_anchos))\n",
    "print('Entropía condicional largos (L1|L2):',conditional_entropy(conditional_prob_largos, joint_prob_largos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Información mutua y entropía\n",
    "\n",
    "$$ I(X;Y) = H(X) - H(X|Y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua clase 1: 0.5300115307683494\n",
      "Información mutua anchos: 0.0796008638042334\n"
     ]
    }
   ],
   "source": [
    "def mutual_information(marg_prob_X, marg_prob_Y, joint_prob) -> float:\n",
    "    I = 0\n",
    "    for x in range(marg_prob_X.shape[0]):\n",
    "        for y in range(marg_prob_Y.shape[0]):\n",
    "            if joint_prob[x][y] != 0:\n",
    "                I += joint_prob[x][y] * np.log2(joint_prob[x][y] / (marg_prob_X[x] * marg_prob_Y[y]))\n",
    "    return I\n",
    "\n",
    "hist_clase1, _, _ = np.histogram2d(largo_clase1, ancho_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "\n",
    "marginal_largo_clase1 = np.sum(joint_prob_clase1, axis=1)\n",
    "marginal_ancho_clase1 = np.sum(joint_prob_clase1, axis=0)\n",
    "\n",
    "print('Información mutua clase 1:',mutual_information(marginal_largo_clase1, marginal_ancho_clase1, joint_prob_clase1))\n",
    "\n",
    "\n",
    "# Lo mismo para ancho1 y ancho2\n",
    "ancho1 = np.array(ancho_clase1)[:ancho_clase2.shape[0]]\n",
    "ancho2 = ancho_clase2\n",
    "\n",
    "hist_anchos, _, _ = np.histogram2d(ancho1, ancho2, bins=5)\n",
    "joint_prob_anchos = hist_anchos / np.sum(hist_anchos)\n",
    "\n",
    "marginal_ancho1 = np.sum(joint_prob_anchos, axis=1)\n",
    "marginal_ancho2 = np.sum(joint_prob_anchos, axis=0)\n",
    "\n",
    "print('Información mutua anchos:',mutual_information(marginal_ancho1, marginal_ancho2, joint_prob_anchos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información mutua I(L1;A1): 0.5300115307683496\n",
      "Información mutua H(L1) - H(L1|A1): 0.5300115307683493\n",
      "Información mutua I(L2;A2): 0.6655652637928242\n",
      "Información mutua H(L2) - H(L2|A2): 0.6655652637928238\n"
     ]
    }
   ],
   "source": [
    "# Corroboración\n",
    "hist_clase1, _, _ = np.histogram2d(ancho_clase1, largo_clase1, bins=5)\n",
    "joint_prob_clase1 = hist_clase1 / np.sum(hist_clase1)\n",
    "conditional_prob_clase1 = conditional_distribution(joint_prob_clase1)\n",
    "I_clase_1 = entropy(prob_largo_clase1) - conditional_entropy(conditional_prob_clase1, joint_prob_clase1)\n",
    "print('Información mutua I(L1;A1):',mutual_information(prob_ancho_clase1, prob_largo_clase1, joint_prob_clase1))\n",
    "print('Información mutua H(L1) - H(L1|A1):', I_clase_1)\n",
    "\n",
    "hist_clase2, _, _ = np.histogram2d(ancho_clase2, largo_clase2, bins=5)\n",
    "joint_prob_clase2 = hist_clase2 / np.sum(hist_clase2)\n",
    "conditional_prob_clase2 = conditional_distribution(joint_prob_clase2)\n",
    "I_clase_2 = entropy(prob_largo_clase2) - conditional_entropy(conditional_prob_clase2, joint_prob_clase2)\n",
    "print('Información mutua I(L2;A2):', I_clase_2)\n",
    "print('Información mutua H(L2) - H(L2|A2):',mutual_information(prob_ancho_clase2, prob_largo_clase2, joint_prob_clase2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropía relativa e información mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_entropy(P: np.ndarray, Q: np.ndarray) -> float:\n",
    "    D = 0\n",
    "    for p in range(P.shape[0]):\n",
    "        for q in range(Q.shape[1]):\n",
    "            if P[p][q] != 0 and Q[p][q] != 0:\n",
    "                D += P[p][q] * np.log2(P[p][q] / Q[p][q])\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropía relativa C1, C2: 0.24180405291993165\n"
     ]
    }
   ],
   "source": [
    "print('Entropía relativa C1, C2:',relative_entropy(joint_prob_clase1, joint_prob_clase2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
